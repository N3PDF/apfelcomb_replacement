#############
Prerequisites
#############

Generating a *theory*, as defined above, requires several files which are
described next.

*pineko.toml*
-------------

You need to provide a *pineko.toml*, that provides all necessary paths to the input and output folders.
This is a standard example:

::

  [paths]
  # inputs
  ymldb = "data/yamldb"
  grids = "data/grids"
  theory_cards = "data/theory_cards"
  operator_card_template = "data/operator_cards/_template.yaml"
  # outputs
  operator_cards = "data/operator_cards"
  ekos = "data/ekos"
  fktables = "data/fktables"

  [paths.logs]
  eko = "logs/eko"
  fk = "logs/fk"

All the relevant inputs are described below.

*ymldb*
-------

You need all files of the *ymldb* [2]_.  [**DEBUG**: Look at the respective *load.sh* script to load from dom.]
This defines the mapping from datasets to FK tables. An actual (rather simple) example is the following:

::

  conversion_factor: 1.0
  operands:
  - - HERA_NC_318GEV_EP_SIGMARED
  operation: 'null'
  target_dataset: HERACOMBNCEP920

In ``operands`` all the necessary FK tables for the ``target_dataset`` are listed. In this case ``operation`` is
``NULL`` which means that the FK tables will just be concatenated but other kinds of operations
can be used (for instance ``ratio``).

Theory Runcards
---------------

You need to provide the necessary theory runcards named with their respective theory ID inside the *paths.theory_cards* folder [3]_.
For more details about theory runcards you can look to https://eko.readthedocs.io/en/latest/code/IO.html under **Theory Runcards**.

Default Operator Card
---------------------

You need to provide a default operator card for |EKO| [4]_.
[**DEBUG**: Look at the respective *load.sh* script to load from dom.]
An example is the following:

::

  ev_op_max_order: 10
  ev_op_iterations: 1
  n_integration_cores: 6
  backward_inversion: "exact"
  Q2grid: [50.]
  interpolation_is_log: True
  interpolation_polynomial_degree: 4
  interpolation_xgrid:
    - 1.9999999999999954e-07
    - 3.034304765867952e-07
    - 4.6035014748963906e-07
    - 6.984208530700364e-07
    - 1.0596094959101024e-06
    - 1.607585498470808e-06
    - 2.438943292891682e-06
    - 3.7002272069854957e-06
    - 5.613757716930151e-06
    - 8.516806677573355e-06
    - 1.292101569074731e-05
    - 1.9602505002391748e-05
    - 2.97384953722449e-05
    - 4.511438394964044e-05
    - 6.843744918967896e-05
    ...
    - 0.31438740076927585
    - 0.3668753186482242
    - 0.4221667753589648
    - 0.4798989029610255
    - 0.5397572337880445
    - 0.601472197967335
    - 0.6648139482473823
    - 0.7295868442414312
    - 0.7956242522922756
    - 0.8627839323906108
    - 0.9309440808717544
    - 1
  debug_skip_non_singlet: False
  debug_skip_singlet: False

For more details about what is needed inside an operator card please refer to https://eko.readthedocs.io/en/latest/code/IO.html
under the section **Operator Runcard**. Note that the actual operator cards for each FK table will be
generated by *pineko* itself starting from this default template.

Grids
-----

*pineko* does **NOT** compute grids, which are instead expected input to *pineko*.
There are typically two ways to obtain grids: computing them from scratch with `runcards <https://github.com/NNPDF/runcards/>`_
or reusing existing ones.

Generate new Grids with *rr*
""""""""""""""""""""""""""""

You need to run *rr* with a given theory runcard and put the generated grid file with the same name
inside the *paths.grids/theory_id* folder. The name has to match the *ymldb* which is the case by default.

Inherit Grids from Existing Theory
""""""""""""""""""""""""""""""""""

You can reuse the grids from a different theory by running::

  pineko theory inherit-grids SOURCE_THEORY_ID TARGET_THEORY_ID DATASET1 DATASET2 ...

The relation between the source theory and the target theory is non-trivial [5]_.


Notes
-----

.. [2] this is to be replaced by the new CommonData format

.. [3] this is to be replaced by a binding to the true theory DB

.. [4] I'm thinking how to improve this, because how could we provide a study on the interpolation accuracy? at the moment there just equal

.. [5] examples being scale variations, different evolution settings, etc.
